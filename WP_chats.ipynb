{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WP_chats.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNeBNVrqcgvetVCNyHCWV4w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudhanshuranjansr/WP/blob/master/WP_chats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-qfY_1mHXAu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "461bf081-5ae0-44da-909e-3424fc54684f"
      },
      "source": [
        "from pandas import Series,DataFrame\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knTBBkfiUbzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import spaCy and load the language library\n",
        "import spacy\n",
        "from nltk.corpus import stopwords \n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOLbIASiUdOO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6c5bd7b2-9ac7-47a6-e5c4-80b586f1c9cb"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Embedding,Dropout\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCgHBeakVQnp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "f7f0d2a4-579e-49d9-d951-501998fefe73"
      },
      "source": [
        "pip install indic-transliteration"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting indic-transliteration\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/c4/4629bf091101592595bfbcac791f8b5475bdbaefd795e5b12589168922a5/indic_transliteration-1.9.6-py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 2.9MB/s \n",
            "\u001b[?25hCollecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from indic-transliteration) (2019.12.20)\n",
            "Collecting backports.functools-lru-cache\n",
            "  Downloading https://files.pythonhosted.org/packages/da/d1/080d2bb13773803648281a49e3918f65b31b7beebf009887a529357fd44a/backports.functools_lru_cache-1.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium->indic-transliteration) (1.24.3)\n",
            "Installing collected packages: selenium, backports.functools-lru-cache, indic-transliteration\n",
            "Successfully installed backports.functools-lru-cache-1.6.1 indic-transliteration-1.9.6 selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMunWHfDGDPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import indic_transliteration\n",
        "from indic_transliteration import sanscript\n",
        "from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C5TJD9sUrJ1",
        "colab_type": "text"
      },
      "source": [
        "**importing data **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcUTio5NUjbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('WhatsApp Chat with Gupta_kapoor prob discuss (1).txt','r') as txt:\n",
        "    train=txt.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6e6X0BYGh_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('Hinglish_to_english2.txt','r') as txt:\n",
        "    he2=txt.readlines()\n",
        "with open('hinglish_abusive.txt','r') as txt:\n",
        "    he_a=txt.readlines()\n",
        "with open('hinglish_to_english1.txt','r') as txt:\n",
        "    he1=txt.readlines()\n",
        "with open('stopwords.txt','r') as txt:\n",
        "    he_s=txt.readlines()\n",
        "with open('s_h_w.txt','r') as txt:\n",
        "    shw=txt.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQwSl7tBUjht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the way we speak ,the words we use tells a lot about our charatcter and nature and can also prdict our response to the questions at the gievn situation\n",
        "#here i'm gonna import the chat from wp and trace the charteers upon the besis of the same \n",
        "#may have to creat a hinglish dict "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBftEKimUjkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function_box\n",
        "\n",
        "def mes_sep(df,n):\n",
        "  a=[]\n",
        "  b=[]\n",
        "  for  i in df.index:\n",
        "        if df[\"Name\"][i]==n:\n",
        "          a=np.append(a,df[\"Messages\"][i])\n",
        "          b=np.append(b,df[\"Time\"][i])\n",
        "  dfab=DataFrame([b,a],index=[\"Time\",\"Message\"]).T\n",
        "  return(dfab)\n",
        "\n",
        "\n",
        "def detect_language(character):\n",
        "    maxchar = max(character)\n",
        "    if u'\\u0900' <= maxchar <= u'\\u097f':\n",
        "        return 'hin'\n",
        "    else:\n",
        "        return 'eng'\n",
        "\n",
        "\n",
        "def separator(mat):                                 #separtes hinglish words from the english ones \n",
        "  mv2=[]\n",
        "  mv1=[]\n",
        "  for i in np.arange(len(mat)-1):\n",
        "      mv=mat[i].split(\",\")\n",
        "      mv11=mv[0]\n",
        "      mv21=mv[1][:-1]\n",
        "      mv2=np.append(mv2,mv21)\n",
        "      mv1=np.append(mv1,mv11)\n",
        "  dfmv=DataFrame([mv1,mv2],index=[\"hinglish\",\"english\"]).T\n",
        "  return(dfmv)\n",
        "\n",
        "def filter(txt):                                    #fiters the dictionary for the given biginninng letter of the Hinglish word\n",
        "    txt1='^'+txt\n",
        "    dfl=dfhe['hinglish'].str.contains(txt1)\n",
        "    dfhe3=dfhe\n",
        "    dfhe3[\"logic\"]=dfl\n",
        "    dfr=dfhe3[dfhe3['logic'] == True]\n",
        "    dfr=dfr.drop(\"logic\",axis=1)\n",
        "    return(dfr)\n",
        "\n",
        "def finder(dc,wrd):                                 #finds a prticular word in  a given dictionary\n",
        "     n=len(dc[\"english\"])\n",
        "     for i in dc.index.values:\n",
        "       if (dc[\"hinglish\"][i] == wrd) or (dc[\"hinglish\"][i] in wrd):\n",
        "         r=dc[\"english\"][i]\n",
        "         break\n",
        "       else:\n",
        "         r=wrd\n",
        "     return(r)\n",
        "  \n",
        "def clss(arr):                                     #classified the postive negative sentiments\n",
        "  p=0\n",
        "  n=0\n",
        "  z=0\n",
        "  n1=len(arr)\n",
        "  for i in arr:\n",
        "    if i>0:\n",
        "      p=p+1\n",
        "    elif i<0:\n",
        "      n=n+1\n",
        "    else:\n",
        "      z=z+1\n",
        "    dfr=DataFrame([(n/n1)*100,(p/n1)*100,(z/n1)*100],index=[\"Negative\",\"Positive\",\"Nutral\"],columns=[\"percent\"])\n",
        "  return(dfr)\n",
        "\n",
        "def hot_words(df_n):        \n",
        "  cr11b=df_n[\"Message\"].values\n",
        "  cr2b=[]\n",
        "  for j in np.arange(len(cr11b)):\n",
        "     txt=cr11b[j].lower()\n",
        "     txt1=\"\"\n",
        "     for  i in txt.split(\" \"):\n",
        "        if i not in (stop_words):\n",
        "           txt1=txt1+\"\".join(i)+\" \"\n",
        "     cr2b=np.append(cr2b,txt1)\n",
        "  dfcr2b=DataFrame(cr2b,columns=[\"Message\"]) \n",
        "  df_n=dfcr2b                     #gives the hot words for the given df for particular person\n",
        "  from sklearn.decomposition import NMF\n",
        "  nmf_model = NMF(n_components=7,random_state=23)\n",
        "  tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "  dtm = tfidf.fit_transform(df_n['Message'])\n",
        "  nmf_model.fit(dtm)\n",
        "  for index,topic in enumerate(nmf_model.components_):\n",
        "    r=([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
        "  return(r)\n",
        "  \n",
        "\n",
        "#time and proportion\n",
        "def tm(df_tm,name):\n",
        "  n=len(df_tm[\"Name\"])\n",
        "  df_tm.index=np.arange(n)\n",
        "  d=[]\n",
        "  for i in np.arange(1,len(df_tm[\"Time\"])):\n",
        "      if (df_tm[\"Name\"][i]==name) and (df_tm[\"Name\"][i-1]!=name):\n",
        "          t1=df_tm[\"Time\"][i]\n",
        "          t0=df_tm[\"Time\"][i-1]\n",
        "          d1=t1-t0\n",
        "          d=np.append(d,d1)\n",
        "      else:\n",
        "        continue\n",
        "      ds=np.mean(d).seconds\n",
        "      dd=(np.mean(d).days)*24*60\n",
        "      dm=ds/60\n",
        "  n1=0\n",
        "  for i in np.arange(n):\n",
        "    if (df_tm[\"Name\"][i]==name):\n",
        "        n1=n1+1\n",
        "  p=(n1/n)*100\n",
        "  return([dm+dd,p])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eplNAFvcUjqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data_cleaning_and_separation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAnC-AyrR_eZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train1=[]\n",
        "for  i in np.arange(len(train)):\n",
        "  if train[i][2:3]==\"/\":\n",
        "    train1=np.append(train1,train[i])\n",
        "train=train1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS5L5qsEdKYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t=[]\n",
        "p2=[]\n",
        "for i in np.arange(2,len(train)):\n",
        "     x12,x11= train[i].split(\" - \",1)\n",
        "     y=int((x12)[6:8])\n",
        "     m=int((x12)[3:5])           \n",
        "     d=int(x12[0:2])            \n",
        "     h=int(x12[9:11])              \n",
        "     mt=int(x12[-5:-3])\n",
        "     pm=x12[-2:]\n",
        "     if pm==\"pm\":\n",
        "        h=h+12\n",
        "     d1=datetime.datetime(y, m, d, h, mt, 0)\n",
        "     t=np.append(t,d1)\n",
        "     p2=np.append(p2,x11)\n",
        "name=[]\n",
        "mes=[]\n",
        "dt=[]\n",
        "for i in np.arange(len(p2)):\n",
        "   x1=p2[i].split(\":\",1)\n",
        "   if len(x1)==2:\n",
        "     name=np.append(name,x1[0])\n",
        "     mes=np.append(mes,x1[1])\n",
        "   else:\n",
        "     dt=np.append(dt,i)\n",
        "dft=DataFrame([t,p2]).T\n",
        "dft=dft.drop(dt,axis=0)\n",
        "dft[\"name\"]=name\n",
        "dft[\"message\"]=mes\n",
        "dft=dft.drop(1,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKONiHef_E2o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "92dc3a23-c7d9-4cd6-f6e9-263aa727c724"
      },
      "source": [
        "dft.columns=[\"Time\",\"Name\",\"Messages\"]\n",
        "dft.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>Name</th>\n",
              "      <th>Messages</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0020-06-24 20:54:00</td>\n",
              "      <td>Sudhanshu Ranjan</td>\n",
              "      <td>भई लोग ham roz gupta ar kapoor ke problems ba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0020-06-24 20:55:00</td>\n",
              "      <td>Sudhanshu Ranjan</td>\n",
              "      <td>Arun bhai ar bhart bahi hamare doubts clear द...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0020-06-24 20:55:00</td>\n",
              "      <td>Shivam Gupta</td>\n",
              "      <td>Or koi bi query ho usko solve kiya jayega\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0020-06-24 21:12:00</td>\n",
              "      <td>Bharat New</td>\n",
              "      <td>Okay .... No doubt\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0020-06-24 21:12:00</td>\n",
              "      <td>Bharat New</td>\n",
              "      <td>Ham matlab kon kon ?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0020-06-24 21:15:00</td>\n",
              "      <td>Shivam Gupta</td>\n",
              "      <td>Jo jo group me h\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0020-06-24 21:16:00</td>\n",
              "      <td>Akm</td>\n",
              "      <td>Well done\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0020-06-24 21:16:00</td>\n",
              "      <td>Akm</td>\n",
              "      <td>Sb mil baat के karenge\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0020-06-24 21:18:00</td>\n",
              "      <td>Bharat New</td>\n",
              "      <td>Ok\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0020-06-26 18:36:00</td>\n",
              "      <td>Sudhanshu Ranjan</td>\n",
              "      <td>&lt;Media omitted&gt;\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Time  ...                                           Messages\n",
              "0  0020-06-24 20:54:00  ...   भई लोग ham roz gupta ar kapoor ke problems ba...\n",
              "1  0020-06-24 20:55:00  ...   Arun bhai ar bhart bahi hamare doubts clear द...\n",
              "2  0020-06-24 20:55:00  ...        Or koi bi query ho usko solve kiya jayega\\n\n",
              "3  0020-06-24 21:12:00  ...                               Okay .... No doubt\\n\n",
              "4  0020-06-24 21:12:00  ...                             Ham matlab kon kon ?\\n\n",
              "5  0020-06-24 21:15:00  ...                                 Jo jo group me h\\n\n",
              "6  0020-06-24 21:16:00  ...                                        Well done\\n\n",
              "7  0020-06-24 21:16:00  ...                           Sb mil baat के karenge\\n\n",
              "8  0020-06-24 21:18:00  ...                                               Ok\\n\n",
              "9  0020-06-26 18:36:00  ...                                  <Media omitted>\\n\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHK59ds2ta7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names=dft[\"Name\"].unique()\n",
        "n2=[]\n",
        "for i in names:\n",
        "  n1=i.split()\n",
        "  for j in n1:\n",
        "      n2=np.append(n2,j.lower())\n",
        "names=n2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAKwRWyhUjzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#some statistics analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdi2Dz6Z-4OJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f7d21d6f-8c62-441b-e4ac-5e155d1c545c"
      },
      "source": [
        "(mes_sep(dft,\"Shivam Gupta\")).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0020-06-24 20:55:00</td>\n",
              "      <td>Or koi bi query ho usko solve kiya jayega\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0020-06-24 21:15:00</td>\n",
              "      <td>Jo jo group me h\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0020-06-26 20:09:00</td>\n",
              "      <td>Kaun sa third\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0020-06-26 20:16:00</td>\n",
              "      <td>Ok\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0020-07-01 13:59:00</td>\n",
              "      <td>Wait bahi usper pahuchate h to btate h\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Time                                       Message\n",
              "0  0020-06-24 20:55:00   Or koi bi query ho usko solve kiya jayega\\n\n",
              "1  0020-06-24 21:15:00                            Jo jo group me h\\n\n",
              "2  0020-06-26 20:09:00                               Kaun sa third\\n\n",
              "3  0020-06-26 20:16:00                                          Ok\\n\n",
              "4  0020-07-01 13:59:00      Wait bahi usper pahuchate h to btate h\\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKJdkBVo_JKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#corpus_cleaning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAe2bAInISZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfhe1=separator(he1)\n",
        "dfhe2=separator(he2)\n",
        "frames = [dfhe1, dfhe2]\n",
        "dfhe=pd.concat(frames)\n",
        "(dfhe.index)=np.arange(0,len(dfhe[\"english\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isbbs5iMrzbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#time_and_proportion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EFLB5-U7kTcy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa112a1c-c997-4e00-c976-6ed9f3a0cf99"
      },
      "source": [
        "tm(dft,\"Akm\")#+tm(dft,\"Akm\")[1]+tm(dft,\"Shivam Gupta\")[1]+tm(dft,\"Sudhanshu Ranjan\")[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[104.36666666666666, 23.981900452488688]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWmeV6LO_JNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4a53529e-8ee8-49a7-fb0f-9f879511fc20"
      },
      "source": [
        "#cr=(dft[\"Messages\"].values)\n",
        "df_cr=(mes_sep(dft,\"Shivam Gupta\"))\n",
        "cr=df_cr[\"Message\"].values\n",
        "cr[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' Or koi bi query ho usko solve kiya jayega\\n',\n",
              "       ' Jo jo group me h\\n', ' Kaun sa third\\n', ' Ok\\n',\n",
              "       ' Wait bahi usper pahuchate h to btate h\\n'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysb5hAqFzNzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('s_h_w.txt','r') as txt:\n",
        "    shw=txt.readlines()\n",
        "shw1=separator(shw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut0XRLWDr7M5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cr1=[]\n",
        "for i in np.arange(len(cr)):\n",
        "  sen1=\"\"\n",
        "  sen=cr[i].split()\n",
        "  n=len(sen)\n",
        "  for j in np.arange(n):\n",
        "     w=sen[j]\n",
        "     lan=detect_language(w)\n",
        "     if lan==\"hin\":\n",
        "       nw=(transliterate(w, sanscript.DEVANAGARI, sanscript.HK))\n",
        "       sen1=sen1+\"\".join(nw)+\" \"\n",
        "     else:\n",
        "       sen1=sen1+\"\".join(w)+\" \"\n",
        "  cr1=np.append(cr1,sen1)\n",
        "he_s1=[]\n",
        "for  i in np.arange(len(he_s)):\n",
        "    he_s1=np.append(he_s1,(he_s[i][:-1]))\n",
        "he_s=he_s1\n",
        "cr11=[]\n",
        "for i in np.arange(len(cr1)):\n",
        "   string = cr1[i]\n",
        "   cleaned = \" \".join(re.findall(\"[a-zA-Z]+\", string))\n",
        "   cr11=np.append(cr11,cleaned)\n",
        "stop_words.update(he_s)\n",
        "stop_words.update(names)\n",
        "stop_words.update(['media omitted','media', 'omitted','https.','okh','feel'])    \n",
        "cr2=[]\n",
        "for j in np.arange(len(cr11)):\n",
        "  txt=cr11[j].lower()\n",
        "  txt1=\"\"\n",
        "  for  i in txt.split(\" \"):\n",
        "     if i not in (stop_words):\n",
        "      txt1=txt1+\"\".join(i)+\" \"\n",
        "  cr2=np.append(cr2,txt1)\n",
        "dfcr2=DataFrame(cr2)\n",
        "d=[]\n",
        "for i in np.arange(len(dfcr2[0])):\n",
        "   if (dfcr2[0][i]==\"\") or (dfcr2[0][i]==\" \") :\n",
        "     d=np.append(d,i)\n",
        "dfcr2=dfcr2.drop(d,axis=0)\n",
        "cr22=dfcr2[0].values  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eImJcNyiEkfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sentiment_part"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRQuS6CtGlYq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "328ae97c-39c2-47ae-a0ed-b964e2ccc528"
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcPkddtFHc7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cr3=[]\n",
        "for j in np.arange(len(cr22)):\n",
        "  txt=cr22[j].lower()\n",
        "  txt1=\"\"\n",
        "  for  i in txt.split(): \n",
        "      if len(i)!=0:\n",
        "         txt2=finder(shw1,i)\n",
        "         txt1=txt1+\"\".join(txt2)+\" \"\n",
        "  cr3=np.append(cr3,txt1)\n",
        "sn=[]\n",
        "for i in np.arange(len(cr3)):\n",
        "  a = cr3[i]\n",
        "  sc=sid.polarity_scores(a)\n",
        "  sc1=(sc).get(\"compound\")\n",
        "  sn=np.append(sn,sc1)\n",
        "dff=DataFrame([sn,cr3],index=[\"Score\",\"Message\"]).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KzSSZYyQBYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_bk=dff\n",
        "#df_sr=dff\n",
        "#df_sg=dff\n",
        "#df_Akm=dff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zolsN4kxWAD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "cd0b3beb-8378-4c51-c173-f09143d2b156"
      },
      "source": [
        "clss(df_bk[\"Score\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>percent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Negative</th>\n",
              "      <td>5.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Positive</th>\n",
              "      <td>19.444444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nutral</th>\n",
              "      <td>75.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            percent\n",
              "Negative   5.555556\n",
              "Positive  19.444444\n",
              "Nutral    75.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dkK5_-pz5q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#topic_modelling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIeJqAsbFuYv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7587ad2a-f198-479f-aa42-4e3ce75fa04e"
      },
      "source": [
        "hot_words(df_bk)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wait', 'limit']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62d9L8UvvdVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#5_feelings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfOkU8darszm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5faa0888-3889-4777-a40b-a37173cc26fd"
      },
      "source": [
        "with open('train.txt','r') as txt:\n",
        "    train=txt.readlines()\n",
        "x=[]\n",
        "y=[]\n",
        "for i in np.arange(len(train)-1):\n",
        "  txt = train[i]\n",
        "  x1,y1 = txt.split(\";\")\n",
        "  x=np.append(x,x1)\n",
        "  y=np.append(y,y1[:-1])\n",
        "import random\n",
        "from sklearn.utils import resample\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "df_im=DataFrame([y,x],index=[\"class\",\"TF\"]).T\n",
        "df_e1=df_im[df_im[\"class\"]==\"love\"]\n",
        "df_e2=df_im[df_im[\"class\"]==\"fear\"]\n",
        "df_e3=df_im[df_im[\"class\"]==\"anger\"]\n",
        "df_e4=df_im[df_im[\"class\"]==\"joy\"] \n",
        "df_e5=df_im[df_im[\"class\"]==\"sadness\"]   \n",
        "df_e11 = DataFrame([np.repeat(\"love\",1000),random.sample(list(df_e1[\"TF\"].values),1000)],index=[\"class\",\"TF\"]).T\n",
        "df_e21 = DataFrame([np.repeat(\"fear\",1000),random.sample(list(df_e2[\"TF\"].values),1000)],index=[\"class\",\"TF\"]).T\n",
        "df_e31 = DataFrame([np.repeat(\"anger\",1000),random.sample(list(df_e3[\"TF\"].values),1000)],index=[\"class\",\"TF\"]).T\n",
        "df_e41 = DataFrame([np.repeat(\"joy\",1000),random.sample(list(df_e4[\"TF\"].values),1000)],index=[\"class\",\"TF\"]).T\n",
        "df_e51 = DataFrame([np.repeat(\"sadness\",1000),random.sample(list(df_e5[\"TF\"].values),1000)],index=[\"class\",\"TF\"]).T\n",
        "df_e=pd.concat([df_e11, df_e21,df_e31,df_e41,df_e51])\n",
        "df_e.index=np.arange(len(df_e[\"class\"].values))\n",
        "nltk.download('wordnet')\n",
        "x1=df_e[\"TF\"].values\n",
        "y1=df_e[\"class\"].values\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size=.01, random_state=42)\n",
        "x1=x_train\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "xn=[]\n",
        "for sen in x1:\n",
        "   word_list = nltk.word_tokenize(sen)\n",
        "   lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "   xn=np.append(xn,lemmatized_output)\n",
        " \n",
        " \n",
        "cr=[]\n",
        "for txt in xn:\n",
        "   txt1=\"\"\n",
        "   for i in txt.split():\n",
        "     if i not in (stop_words):\n",
        "      txt1=txt1+\"\".join(i)+\" \"\n",
        "   cr=np.append(cr,txt1)\n",
        " \n",
        "y=[]\n",
        "for i in y_train:\n",
        "  if i==\"anger\":\n",
        "    y=np.append(y,0)\n",
        "  elif i==\"fear\":\n",
        "    y=np.append(y,1)\n",
        "  elif i==\"joy\":\n",
        "    y=np.append(y,2)\n",
        "  elif i==\"love\":\n",
        "    y=np.append(y,3)\n",
        "  else:\n",
        "    y=np.append(y,4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge48ShFJoBVL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "e0772551-6ad5-4f7c-e09e-ede06387393f"
      },
      "source": [
        "def model_tr():\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    model=DecisionTreeClassifier()\n",
        "    return(model)\n",
        "pmodel = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "                       ('mo', model_tr())])\n",
        "pmodel.fit(cr,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['https', 'mo', 'mon'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words={'', 'a', 'aa', 'aad', 'aadi',\n",
              "                                             'aaj', 'aap', 'aapn', 'aapne',\n",
              "                                             'aat'...\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('mo',\n",
              "                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
              "                                        criterion='gini', max_depth=None,\n",
              "                                        max_features=None, max_leaf_nodes=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        presort='deprecated', random_state=None,\n",
              "                                        splitter='best'))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrICMhLbe3ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#functional approach"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hlQrhtfFq_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_cln(mat):\n",
        "  train=mat\n",
        "  train1=[]\n",
        "  for  i in np.arange(len(train)):\n",
        "    if train[i][2:3]==\"/\":\n",
        "       train1=np.append(train1,train[i])\n",
        "  train=train1\n",
        "  t=[]\n",
        "  p2=[]\n",
        "  for i in np.arange(2,len(train)):\n",
        "     x12,x11= train[i].split(\" - \",1)\n",
        "     y=int((x12)[6:8])\n",
        "     m=int((x12)[3:5])           \n",
        "     d=int(x12[0:2])            \n",
        "     h=int(x12[9:11])              \n",
        "     mt=int(x12[-5:-3])\n",
        "     pm=x12[-2:]\n",
        "     if pm==\"pm\":\n",
        "        h=h+12\n",
        "     d1=datetime.datetime(y, m, d, h, mt, 0)\n",
        "     t=np.append(t,d1)\n",
        "     p2=np.append(p2,x11)\n",
        "  name=[]\n",
        "  mes=[]\n",
        "  dt=[]\n",
        "  for i in np.arange(len(p2)):\n",
        "    x1=p2[i].split(\":\",1)\n",
        "    if len(x1)==2:\n",
        "      name=np.append(name,x1[0])\n",
        "      mes=np.append(mes,x1[1])\n",
        "    else:\n",
        "      dt=np.append(dt,i)\n",
        "  dft=DataFrame([t,p2]).T\n",
        "  dft=dft.drop(dt,axis=0)\n",
        "  dft[\"name\"]=name\n",
        "  dft[\"message\"]=mes\n",
        "  dft=dft.drop(1,axis=1)\n",
        "  dft.columns=[\"Time\",\"Name\",\"Messages\"]\n",
        "  return(dft)\n",
        "\n",
        "\n",
        "def cln_sn(cr,he_s,names):\n",
        "    cr1=[]\n",
        "    for i in np.arange(len(cr)):\n",
        "       sen1=\"\"\n",
        "       sen=cr[i].split()\n",
        "       n=len(sen)\n",
        "       for j in np.arange(n):\n",
        "         w=sen[j]\n",
        "         lan=detect_language(w)\n",
        "         if lan==\"hin\":\n",
        "            nw=(transliterate(w, sanscript.DEVANAGARI, sanscript.HK))\n",
        "            sen1=sen1+\"\".join(nw)+\" \"\n",
        "         else:\n",
        "            sen1=sen1+\"\".join(w)+\" \"\n",
        "       cr1=np.append(cr1,sen1)\n",
        "    he_s1=[]\n",
        "    for  i in np.arange(len(he_s)):\n",
        "        he_s1=np.append(he_s1,(he_s[i][:-1]))\n",
        "    he_s=he_s1\n",
        "\n",
        "    cr11=[]\n",
        "    for i in np.arange(len(cr1)):\n",
        "         string = cr1[i]\n",
        "         cleaned = \" \".join(re.findall(\"[a-zA-Z]+\", string))\n",
        "         cr11=np.append(cr11,cleaned)\n",
        "    stop_words.update(he_s)\n",
        "    stop_words.update(names)\n",
        "    stop_words.update(['media omitted','media', 'omitted','https.','okh']) \n",
        "    \n",
        "    cr2=[]\n",
        "    for j in np.arange(len(cr11)):\n",
        "        txt=cr11[j].lower()\n",
        "        txt1=\"\"\n",
        "        for  i in txt.split(\" \"):\n",
        "           if i not in (stop_words):\n",
        "             txt1=txt1+\"\".join(i)+\" \"\n",
        "             cr2=np.append(cr2,txt1)\n",
        "    dfcr2=DataFrame(cr2)\n",
        "\n",
        "    d=[]\n",
        "    for i in np.arange(len(dfcr2[0])):\n",
        "       if (dfcr2[0][i]==\"\") or (dfcr2[0][i]==\" \") :\n",
        "          d=np.append(d,i)\n",
        "    dfcr2=dfcr2.drop(d,axis=0)\n",
        "    cr22=dfcr2[0].values \n",
        "    \n",
        "    cr3=[]\n",
        "    for j in np.arange(len(cr22)):\n",
        "       txt=cr22[j].lower()\n",
        "       txt1=\"\"\n",
        "       for  i in txt.split(): \n",
        "         if len(i)!=0:\n",
        "            txt2=finder(shw1,i)\n",
        "            txt1=txt1+\"\".join(txt2)+\" \"\n",
        "       cr3=np.append(cr3,txt1)\n",
        "    sn=[]\n",
        "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    for i in np.arange(len(cr3)):\n",
        "      a = cr3[i]\n",
        "      sc=sid.polarity_scores(a)\n",
        "      sc1=(sc).get(\"compound\")\n",
        "      sn=np.append(sn,sc1)\n",
        "    dff=DataFrame([sn,cr3],index=[\"Score\",\"Message\"]).T\n",
        "    return(dff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA1eCfdMGlHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fact(mat,name,he_s):\n",
        "  dft=data_cln(mat)\n",
        "  #names=dft[\"Name\"].unique()\n",
        "  names=dft[\"Name\"].unique()\n",
        "  n2=[]\n",
        "  for i in names:\n",
        "     n1=i.split()\n",
        "     for j in n1:\n",
        "        n2=np.append(n2,j.lower())\n",
        "  names=n2\n",
        "  cr=(mes_sep(dft,name))\n",
        "  cr=cr[\"Message\"].values\n",
        "  df_bk=cln_sn(cr,he_s,name)\n",
        "  mn=np.mean(df_bk[\"Score\"])\n",
        "  sd=np.sqrt(np.var(df_bk[\"Score\"]))\n",
        "  df_bk1=[]\n",
        "  df_cr=df_bk[\"Message\"].values\n",
        "  for j in np.arange(len(df_cr)):\n",
        "      txt=df_cr[j].lower()\n",
        "      txt1=\"\"\n",
        "      for  i in txt.split(\" \"):\n",
        "          if i not in (stop_words):\n",
        "            txt1=txt1+\"\".join(i)+\" \"\n",
        "            df_bk1=np.append(df_bk1,txt1)\n",
        "  df_bk22=DataFrame(df_bk1,columns=[\"Message\"])\n",
        "\n",
        "  p=\"\"\n",
        "  s=\"\"\n",
        "  if mn>0:\n",
        "    p=\"postive\"\n",
        "  elif mn<0:\n",
        "    p=\"negative\"\n",
        "  else:\n",
        "    p=\"nutral\"\n",
        "\n",
        "  if mn>mn+.3*sd or mn<mn-.3*sd:\n",
        "    s=\"highly\"\n",
        "  else:\n",
        "    s=\"slightly\"\n",
        "  c=s+\" \"+p\n",
        "  t,p=tm(dft,name)\n",
        "  em5=pmodel.predict(df_bk22[\"Message\"].values)\n",
        "  e1=0;e2=0;e3=0;e4=0;e5=0;\n",
        "  for i in em5:\n",
        "     if i==0:\n",
        "       e1=e1+1\n",
        "     elif i==1:\n",
        "        e2=e2+1\n",
        "     elif i==3:\n",
        "        e3=e3+1\n",
        "     elif i==4:\n",
        "        e4=e4+1\n",
        "     else :\n",
        "        e5=e5+1\n",
        "  en=len(em5)\n",
        "  dfem=DataFrame([e1/en,e2/en,e3/en,e4/en,e5/en],index=[\"Anger\",\"Fear\",\"Joy\",\"Love\",\"Sadness\"],columns=[\"proportion\"])\n",
        "  #return([clss(df_bk[\"Score\"]),\"\\n Most frequentlt used words by \",name,\"is\",hot_words(df_bk)])\n",
        "  print(\"\\nThe average time taken by \",name,\" to reply is \",t,\"min.\")\n",
        "  print(\"\\n The percentage of \",name,\"'s chats in total chat is \",p)\n",
        "  print(\"\\nThe nature of \",name,\"using the  whatsapp chat data is given as \",\"\\n\\n\" ,clss(df_bk[\"Score\"]))\n",
        "  print(\"\\n\",name,\"is \",c,\"person. And the mean score is \",mn)\n",
        "  print(\"\\n The most frquently used words by \",name, \"are \",hot_words(df_bk22))\n",
        "  #print(\"\\n The furthur classification of sentiments can be \\n\",dfem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3cmvbGwPP0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('WhatsApp Chat with Aliva.txt','r') as txt:\n",
        "    train=txt.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNfkA747mU3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "840bdd7b-ce45-44a0-f8c5-dc5dbe3208c3"
      },
      "source": [
        "with open('s_h_w.txt','r') as txt:\n",
        "    shw=txt.readlines()\n",
        "shw1=separator(shw)\n",
        "fact(train,\"Aliva\",he_s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The average time taken by  Aliva  to reply is  19.033333333333335 min.\n",
            "\n",
            " The percentage of  Aliva 's chats in total chat is  47.55793805501408\n",
            "\n",
            "The nature of  Aliva using the  whatsapp chat data is given as  \n",
            "\n",
            "             percent\n",
            "Negative   1.287976\n",
            "Positive  36.879363\n",
            "Nutral    61.832661\n",
            "\n",
            " Aliva is  slightly postive person. And the mean score is  0.1563223773473632\n",
            "\n",
            " The most frquently used words by  Aliva are  ['gc', 'ldki', 'kat', 'milti', 'aim', 'wla', 'pas', 'college', 'glti', 'ayi', 'test', 'lg', 'leke', 'cl', 'gyi']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP5LGk-OFsPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_cln(mat):\n",
        "  train=mat\n",
        "  train1=[]\n",
        "  for  i in np.arange(len(train)):\n",
        "    if train[i][2:3]==\"/\":\n",
        "       train1=np.append(train1,train[i])\n",
        "  train=train1\n",
        "  t=[]\n",
        "  p2=[]\n",
        "  for i in np.arange(2,len(train)):\n",
        "     x12,x11= train[i].split(\" - \",1)\n",
        "     y=int((x12)[6:8])\n",
        "     m=int((x12)[3:5])           \n",
        "     d=int(x12[0:2])            \n",
        "     h=int(x12[9:11])              \n",
        "     mt=int(x12[-5:-3])\n",
        "     pm=x12[-2:]\n",
        "     if pm==\"pm\":\n",
        "        h=h+12\n",
        "     d1=datetime.datetime(y, m, d, h, mt, 0)\n",
        "     t=np.append(t,d1)\n",
        "     p2=np.append(p2,x11)\n",
        "  name=[]\n",
        "  mes=[]\n",
        "  dt=[]\n",
        "  for i in np.arange(len(p2)):\n",
        "    x1=p2[i].split(\":\",1)\n",
        "    if len(x1)==2:\n",
        "      name=np.append(name,x1[0])\n",
        "      mes=np.append(mes,x1[1])\n",
        "    else:\n",
        "      dt=np.append(dt,i)\n",
        "  dft=DataFrame([t,p2]).T\n",
        "  dft=dft.drop(dt,axis=0)\n",
        "  dft[\"name\"]=name\n",
        "  dft[\"message\"]=mes\n",
        "  dft=dft.drop(1,axis=1)\n",
        "  dft.columns=[\"Time\",\"Name\",\"Messages\"]\n",
        "  return(dft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf9o61LAGWbV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b5da172-f62c-499d-969e-648b887bbe72"
      },
      "source": [
        "#train[8780].split(\" - \",1)\n",
        "j"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8780"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMURpjaVel5P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81037bb0-ed8c-474e-f128-34d5b4fc7921"
      },
      "source": [
        "#data_cln(train)\n",
        "train1=[]\n",
        "for  i in np.arange(len(train)):\n",
        "  if train[i][2:3]==\"/\":\n",
        "    train1=np.append(train1,train[i])\n",
        "train=train1\n",
        "t=[]\n",
        "p2=[]\n",
        "for i in np.arange(2,len(train)):\n",
        "    x12,x11= train[i].split(\" - \",1)\n",
        "    j=i\n",
        "print(j)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ8eGQ4p_JQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#thrash_box\n",
        "p=[\",\",\" - \",\": \"]\n",
        "x12,x11,x01= train[134].split(\" - \")\n",
        "#x12[4]     #name\n",
        "#x12[1]     #time\n",
        "#x12[4:]     #sentence\n",
        "y=(x12)[6:8]\n",
        "m=(x12)[3:5]           \n",
        "d=x12[0:2]            \n",
        "h=x12[9:11]              #ih hr in PM add 12 there\n",
        "mt=x12[-5:-3]\n",
        "pm=x12[-2:]\n",
        "[y,m,d,h,mt,pm]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#pip install indic-transliteration\n",
        "import indic_transliteration\n",
        "from indic_transliteration import sanscript\n",
        "from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate\n",
        "data = 'हाल की एक स्टडी में पता चला है कि बीड़ी उद्योग में लगे लोगों की संख्या में जिसमें महिलाएं अधिक हैं, राष्ट्रीय स्तर पर इज़ाफा हुआ है लेकिन दक्षिणी सूबों में इसमें गिरावट दर्ज की गई है'\n",
        "print(transliterate(data, sanscript.DEVANAGARI, sanscript.HK))\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.utils import resample\n",
        "df_im=DataFrame([y,x],index=[\"class\",\"TF\"]).T\n",
        "df_min1=df_im[df_im[\"class\"]==\"surprise\"]\n",
        "df_min2=df_im[df_im[\"class\"]==\"love\"]\n",
        "df_min3=df_im[df_im[\"class\"]==\"fear\"]\n",
        "df_min4=df_im[df_im[\"class\"]==\"anger\"]\n",
        "df_maj1=df_im[df_im[\"class\"]==\"joy\"] \n",
        "df_maj2=df_im[df_im[\"class\"]==\"sadness\"]   \n",
        "df_min11 = resample(df_min1, \n",
        "                                 replace=True,     \n",
        "                                 n_samples=2000,    \n",
        "                                 random_state=250)\n",
        "df_min22 = resample(df_min2, \n",
        "                                 replace=True,     \n",
        "                                 n_samples=3000,    \n",
        "                                 random_state=20)\n",
        "df_min33 = resample(df_min3, \n",
        "                                 replace=True,     \n",
        "                                 n_samples=5000,    \n",
        "                                 random_state=212)\n",
        "           \n",
        "df_min44 = resample(df_min4, \n",
        "                                 replace=True,     \n",
        "                                 n_samples=5000,    \n",
        "                                 random_state=1120)\n",
        "df_sm=pd.concat([df_min11, df_min22,df_min33,df_min44,df_maj1,df_maj2])\n",
        "df_sm.index=np.arange(len(df_sm[\"TF\"].values))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "xn=[]\n",
        "for sen in x:\n",
        "   word_list = nltk.word_tokenize(sen)\n",
        "   lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "   xn=np.append(xn,lemmatized_output)\n",
        "\n",
        "\n",
        "cr=[]\n",
        "for txt in xn:\n",
        "   txt1=\"\"\n",
        "   for i in txt.split():\n",
        "     if i not in (stop_words):\n",
        "      txt1=txt1+\"\".join(i)+\" \"\n",
        "   cr=np.append(cr,txt1)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "xnn= vectorizer.fit_transform(cr) \n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(cr)\n",
        "vocabulary_size = len(tokenizer.word_counts)\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "oh=[one_hot(words,vocabulary_size)for  words in cr]\n",
        "sen_len=30\n",
        "e_doc=pad_sequences(oh,padding=\"pre\",maxlen=sen_len)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_senti=DataFrame([clss(df_Akm[\"Score\"])[\"percent\"].values,clss(df_sg[\"Score\"])[\"percent\"].values,\n",
        "           clss(df_bk[\"Score\"])[\"percent\"].values,clss(df_sr[\"Score\"])[\"percent\"].values],\n",
        "          index=[\"Akm\",\"shivam\",\"bharat\",\"SR\"],columns=[\"negative\",\"postive\",\"nutral\"]).T\n",
        "mn=[np.mean(df_Akm[\"Score\"]),np.mean(df_sg[\"Score\"]),\n",
        "                           np.mean(df_bk[\"Score\"]),np.mean(df_sr[\"Score\"])]\n",
        "sd=[np.sqrt(np.var(df_Akm[\"Score\"])),np.sqrt(np.var(df_sg[\"Score\"])),\n",
        "    np.sqrt(np.var(df_bk[\"Score\"])),np.sqrt(np.var(df_sr[\"Score\"]))]\n",
        "df_st=DataFrame([mn,sd],columns=[\"Akm\",\"shivam\",\"bharat\",\"SR\"],index=[\"mean_score\",\"sd_of_score\"])\n",
        "pd.concat([df_senti.T,df_st]).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQXI1bcF_JGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}